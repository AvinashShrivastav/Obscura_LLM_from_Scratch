# Obscura: Building an LLM from Scratch

This repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the "Attention Is All You Need" paper.

## Chapter 1: Initial Setup and Dependencies

This initial commit focuses on setting up the development environment and installing necessary dependencies.

### Dependencies Installed:

* `uv`: A fast Python package installer and resolver.

## Chapter 2: Working with Text Data

This chapter begins the process of preparing text data for model training.

### Initial Text Processing:

* **Data Loading:** Loaded the raw text content from the "Attention Is All You Need" paper.
* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.
* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.
* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.
* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.
* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.

### Vocabulary Creation and Custom Tokenizers:

* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.
* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.
* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.
* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).
* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.
* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.

### Data Loading with Tiktoken and PyTorch Datasets:

* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.
* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).
* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.
* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.
* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.

### Implementing Embeddings:

* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.
* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.
* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.
