import nbformat
import os
import subprocess
import json
from datetime import datetime, timedelta
import random

# --- Configuration ---
MAIN_NOTEBOOK = "Obscura_Building_LLM_from_Scratch_Main.ipynb"
COMMIT_NOTEBOOK = "Obscura_Commit.ipynb"
COMMIT_MESSAGES_FILE = "commit_msg.txt"
README_UPDATES_FILE = "readme_updates.json" # This remains the target file name

# --- README Content as a Python Dictionary (This will be used to GENERATE readme_updates.json if needed) ---
# Note: The script will NOT automatically update README.md or generate readme_updates.json
# if the relevant lines in automate_git_push are commented out.
README_DATA_FOR_GENERATION = [
  {
    "day": 1,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n"
  },
  {
    "day": 2,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n"
  },
  {
    "day": 3,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n"
  },
  {
    "day": 4,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n"
  },
  {
    "day": 5,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n### Implementing Embeddings:\n\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\n\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\n\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\n\n"
  },
  {
    "day": 6,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n### Implementing Embeddings:\n\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\n\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\n\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\n\n## Chapter 3: Implementing Attention Mechanism\n\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\n\n### Core Self-Attention Logic:\n\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\n\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\n\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\n\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\n\n"
  },
  {
    "day": 7,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\sqrt{d_k}$) to prevent vanishing gradients.\\n\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\\n"
  },
  {
    "day": 8,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\sqrt{d_k}$) to prevent vanishing gradients.\\n\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\\n### Regularization with Dropout:\\n\\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n\n"
  },
  {
    "day": 9,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\sqrt{d_k}$) to prevent vanishing gradients.\\n\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\\n### Regularization with Dropout:\n\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n\n### Stacking Multiple Single-Head Attention Layers (Multi-Head Attention Wrapper):\n\n* **`MultiHeadAttentionWrapper` Class:** Implemented a wrapper class that allows for stacking multiple independent `CausalAttention` heads.\\n\n* **Concatenation of Heads:** Showcased how the output context vectors from each individual attention head are concatenated along the last dimension to form a richer, combined representation.\\n\n* **Conceptual Multi-Head:** This serves as an initial step towards understanding multi-head attention, where different heads can learn to focus on different aspects of the input sequence.\\n\n"
  },
  {
    "day": 10,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input input sequence predicts the next token.\\n\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\sqrt{d_k}$) to prevent vanishing gradients.\\n\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\\n### Regularization with Dropout:\\n\\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n\n### Multi-Head Attention:\\n\\n* **`MultiHeadAttentionWrapper` (Conceptual):** An initial approach to multi-head attention by simply stacking multiple independent `CausalAttention` heads and concatenating their outputs. This illustrates the idea of multiple \\\"perspectives.\\\"\\n\n* **Efficient `MultiHeadAttention` Class:** Implemented the more standard and efficient `MultiHeadAttention` class. This class projects the input into a single large Q, K, and V matrix, then implicitly splits these into multiple heads using `view` and `transpose` operations.\\n\n* **Combined Output Projection:** Included an `out_proj` linear layer to combine the concatenated outputs from all attention heads back into the desired output dimension.\\n\n* **Demonstration:** Verified the functionality of the `MultiHeadAttention` class with sample batch data, confirming the correct output shape and demonstrating the parallel computation across heads.\\n\n"
  },
  {
    "day": 11,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 4: Coding an LLM Architecture\n\nThis chapter begins the implementation of the full GPT (Generative Pre-trained Transformer) model architecture, combining all the components developed so far.\n\n### GPT Model Structure (Dummy Implementation):\n\n* **`GPT_CONFIG_124M`:** Defined a configuration dictionary for a GPT-like model with 124 million parameters (similar to GPT-2 Small), specifying `vocab_size`, `context_length`, `emb_dim`, `n_heads`, `n_layers`, `drop_rate`, and `qkv_bias`.\n\n* **`DummyGPTModel`:** Created a placeholder `GPTModel` class that outlines the overall architecture: token embeddings, positional embeddings, dropout, a sequence of Transformer blocks, final layer normalization, and an output head. This initial version uses `DummyTransformerBlock` and `DummyLayerNorm` which simply pass through their inputs.\n\n* **`DummyTransformerBlock`:** A placeholder class for the Transformer block, returning its input directly.\n\n* **`DummyLayerNorm`:** A placeholder class for Layer Normalization, returning its input directly.\n\n* **Model Instantiation and Forward Pass:** Demonstrated instantiating `DummyGPTModel` with the defined configuration and performed a forward pass with sample batched token IDs to verify output shape.\n\n"
  },
  {
    "day": 12,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 4: Coding an LLM Architecture\n\nThis chapter continues the implementation of the full GPT architecture.\n\n### Layer Normalization and GELU Activation:\n\n* **Layer Normalization (Manual & Class):** Explored the concept of Layer Normalization by manually calculating mean and variance across the feature dimension for a batch of inputs. Implemented a custom `LayerNorm` class that correctly applies normalization, including learnable scale and shift parameters, and ensures numerical stability with an epsilon value. Verified that the normalized outputs have mean 0 and variance 1.\n\n* **GELU Activation Function (Manual & Class):** Introduced the Gaussian Error Linear Unit (GELU) activation function, which is commonly used in Transformer models. Implemented a custom `GELU` class and visualized its behavior compared to ReLU, highlighting its smooth, non-linear properties.\n\n* **`FeedForward` Network:** Implemented the `FeedForward` network as a separate module, consisting of two linear layers with a GELU activation in between. This module processes each token's embedding independently.\n\n"
  },
  {
    "day": 13,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 4: Coding an LLM Architecture\n\nThis chapter continues to build out the core components of the Transformer.\n\n### Residual Connections (Shortcut Connections):\n\n* **Demonstration of Vanishing Gradients:** Illustrated the problem of vanishing gradients in deep neural networks without shortcut connections by printing the mean absolute gradients of weights in a simple deep network. Observed how gradients diminish in earlier layers.\n\n* **Impact of Shortcut Connections:** Demonstrated how adding residual (shortcut) connections significantly improves gradient flow, leading to larger and more stable gradients in earlier layers. This is crucial for training very deep models like Transformers.\n\n* **Conceptual Integration:** Explained how residual connections are applied in Transformer blocks by adding the input of a sub-layer to its output, allowing gradients to flow more directly through the network.\n\n"
  },
  {
    "day": 14,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 4: Coding an LLM Architecture\n\nThis chapter integrates the attention and feed-forward networks into a complete Transformer Block.\n\n### Transformer Block Implementation:\n\n* **`TransformerBlock` Class:** Implemented the full `TransformerBlock` module, which is the fundamental building block of the Transformer architecture. This block encapsulates:\n    * **Multi-Head Attention:** The `MultiHeadAttention` module from Chapter 3.\n    * **Feed-Forward Network:** The `FeedForward` module from Day 12.\n    * **Layer Normalization:** Two `LayerNorm` layers applied before the attention and feed-forward sub-layers.\n    * **Residual Connections:** Shortcut connections implemented around both the attention and feed-forward sub-layers, adding the input of each sub-layer to its output.\n    * **Dropout:** Dropout layers applied after the attention and feed-forward outputs for regularization.\n\n* **Block Integration Test:** Demonstrated instantiating a `TransformerBlock` and passing a sample input through it to verify that the input and output shapes remain consistent, as expected in a residual block.\n\n"
  },
  {
    "day": 15,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 4: Coding an LLM Architecture\n\nThis final part of the chapter brings all components together into the complete GPT Model and tests text generation.\n\n### Complete GPT Model and Text Generation:\n\n* **`GPTModel` Class:** Implemented the full `GPTModel` class, which orchestrates all the components:\n    * **Token Embeddings:** `nn.Embedding` for converting token IDs to vectors.\n    * **Positional Embeddings:** `nn.Embedding` for encoding token positions.\n    * **Dropout:** Applied to the combined embeddings.\n    * **Transformer Blocks:** A sequential stack of `TransformerBlock` instances.\n    * **Final Layer Normalization:** Applied before the output layer.\n    * **Output Head:** A linear layer that projects the final embeddings to the vocabulary size, predicting the next token.\n\n* **Parameter Counting:** Calculated and printed the total number of parameters in the `GPTModel`, demonstrating its scale (e.g., 124M parameters for the GPT-2 Small configuration). Also showed how weight tying (sharing weights between token embedding and output layers) reduces the number of *trainable* parameters.\n\n* **Model Size Calculation:** Estimated the model's memory footprint in megabytes.\n\n* **Simple Text Generation Function (`generate_text_simple`):** Implemented a basic greedy text generation function. This function takes a starting sequence, iteratively predicts the next token with the highest probability, and appends it to the sequence. It also includes a mechanism to crop the input context to the model's `context_length`.\n\n* **Text Generation Demonstration:** Used the `GPTModel` (even though it's untrained) and the `generate_text_simple` function to generate a short sequence of text based on a starting prompt. This demonstrates the end-to-end flow of the LLM, from input tokenization to output generation.\n\n"
  }
]

# --- Helper Functions ---
def run_git_command(command, date_str=None):
    """Executes a git command with optional date overriding."""
    env = os.environ.copy()
    if date_str:
        env["GIT_AUTHOR_DATE"] = date_str
        env["GIT_COMMITTER_DATE"] = date_str
    try:
        result = subprocess.run(command, check=True, shell=True, capture_output=True, text=True, env=env)
        print(f"STDOUT:\n{result.stdout}")
        if result.stderr:
            print(f"STDERR:\n{result.stderr}")
    except subprocess.CalledProcessError as e:
        print(f"Error executing command: {e.cmd}")
        print(f"STDOUT:\n{e.stdout}")
        print(f"STDERR:\n{e.stderr}")
        exit(1)

def extract_day_code_cumulative(notebook_path, current_day):
    """
    Extracts code cells cumulatively up to the specified current_day.
    This means it includes all cells from Day 1 up to and including current_day's marked blocks.
    """
    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = nbformat.read(f, as_version=4)

    cumulative_cells = []
    # Track if we are currently inside a day's code block
    # and which day's block we are looking for the end of.
    active_day_block = 0

    for cell in notebook.cells:
        if cell.cell_type == 'code':
            source_lines = cell.source.splitlines()
            
            # Check for start tag for any day up to current_day
            for i in range(1, current_day + 1):
                if f"### Day {i} Code Start ###" in source_lines:
                    active_day_block = i
                    break # Found a start tag, no need to check other days for this cell

            if active_day_block > 0 and active_day_block <= current_day:
                # If we are in an active day block that's relevant to the current commit day,
                # then include this cell, but remove the markers.
                cleaned_source = []
                for line in source_lines:
                    if not (line.strip().startswith("### Day") and line.strip().endswith("###")):
                        cleaned_source.append(line)
                new_cell = nbformat.v4.new_code_cell("\n".join(cleaned_source))
                # Preserve outputs if they exist
                if 'outputs' in cell:
                    new_cell['outputs'] = cell['outputs']
                if 'execution_count' in cell:
                    new_cell['execution_count'] = cell['execution_count']
                cumulative_cells.append(new_cell)

            # Check for end tag of the currently active day block
            if f"### Day {active_day_block} Code End ###" in source_lines:
                active_day_block = 0 # End the active block


        elif cell.cell_type == 'markdown':
            # Include relevant markdown cells for context, assuming they are general chapter headers
            # or introductory text for a chapter that applies to the current day's content.
            # This logic might need refinement if markdown cells are very specific to a single day.
            if "Chapter 1:" in cell.source and current_day >= 1:
                cumulative_cells.append(cell)
            elif "Chapter 2:" in cell.source and current_day >= 2:
                cumulative_cells.append(cell)
            elif "Chapter 3:" in cell.source and current_day >= 3:
                cumulative_cells.append(cell)
            elif "Chapter 4:" in cell.source and current_day >= 11: # Chapter 4 starts from Day 11
                cumulative_cells.append(cell)

    return cumulative_cells

def update_commit_notebook(extracted_cells):
    """Creates/updates the commit notebook with extracted cells."""
    if not extracted_cells:
        print("No cells to add for this day's commit notebook.")
        new_commit_nb = nbformat.v4.new_notebook()
    else:
        new_commit_nb = nbformat.v4.new_notebook()
        new_commit_nb.cells = extracted_cells

    with open(COMMIT_NOTEBOOK, 'w', encoding='utf-8') as f:
        nbformat.write(new_commit_nb, f)
    print(f"Updated {COMMIT_NOTEBOOK} with day's code.")


def get_commit_details(day_number):
    """Reads commit message and date from commit_msg.txt."""
    with open(COMMIT_MESSAGES_FILE, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # Find the block for the current day
    start_marker = f"### Day {day_number} Commit Details Start ###"
    end_marker = f"### Day {day_number} Commit Details End ###"

    in_block = False
    commit_date_str = None
    commit_message_lines = []

    for line in lines:
        if start_marker in line:
            in_block = True
            continue
        if end_marker in line:
            in_block = False
            break
        if in_block:
            if line.strip().startswith("GIT_AUTHOR_DATE="):
                # Extract the date string from the git command line
                parts = line.split('GIT_AUTHOR_DATE="', 1)
                if len(parts) > 1:
                    commit_date_str = parts[1].split('"')[0] # Get content inside first double quotes
            elif line.strip().startswith('git commit -m "'):
                # Extract the message from the git commit command
                message_part = line.split(' -m "', 1)[1].strip()
                # Remove the trailing " and any remaining command parts
                if message_part.endswith('"'):
                    message_part = message_part[:-1]
                commit_message_lines.append(message_part)
            else:
                # Append other lines as part of the multi-line commit message
                commit_message_lines.append(line.strip())

    if not commit_date_str or not commit_message_lines:
        raise ValueError(f"Could not find commit details for Day {day_number} in {COMMIT_MESSAGES_FILE}")

    # Join message lines, handling newlines for multiline messages
    full_commit_message = "\n".join(commit_message_lines).strip()
    return commit_date_str, full_commit_message


def generate_readme_json():
    """Generates the readme_updates.json file from README_DATA_FOR_GENERATION."""
    print(f"Generating {README_UPDATES_FILE}...")
    with open(README_UPDATES_FILE, 'w', encoding='utf-8') as f:
        json.dump(README_DATA_FOR_GENERATION, f, indent=2)
    print(f"Successfully generated {README_UPDATES_FILE}.")

def get_readme_content(day_number):
    """Reads README content for a specific day from readme_updates.json."""
    # Ensure the JSON file exists and is correctly generated before reading
    if not os.path.exists(README_UPDATES_FILE):
        generate_readme_json() # Generate it if it doesn't exist

    with open(README_UPDATES_FILE, 'r', encoding='utf-8') as f:
        readme_data = json.load(f)

    # Find the readme content for the given day number
    for entry in readme_data:
        if entry["day"] == day_number:
            return entry["content"]
    raise ValueError(f"No README content found for Day {day_number} in {README_UPDATES_FILE}")

# --- Main Automation Logic ---
def automate_git_push(start_day=1, end_day=15): # Changed end_day to 15
    # IMPORTANT: Commenting out README generation as per user request.
    # If you later want to generate/update README via script, uncomment this.
    # generate_readme_json()

    for day in range(start_day, end_day + 1):
        print(f"\n--- Processing Day {day} ---")

        # 1. Extract cumulative code for the commit notebook
        all_cells_up_to_current_day = extract_day_code_cumulative(MAIN_NOTEBOOK, day)
        update_commit_notebook(all_cells_up_to_current_day)

        # 2. Update README.md (Commenting out as per user request)
        # readme_content = get_readme_content(day)
        # with open("README.md", "w", encoding="utf-8") as f:
        #     f.write(readme_content)
        # print(f"Updated README.md for Day {day}.")

        # 3. Get commit message and date
        commit_date_str, commit_message = get_commit_details(day)
        print(f"Commit Message for Day {day}:\n{commit_message}")
        print(f"Commit Date for Day {day}: {commit_date_str}")

        # 4. Git add, commit, and push
        print("\nRunning Git commands...")
        run_git_command("git add .") # Add all changes (Obscura_Commit.ipynb, README.md if uncommented)
        run_git_command(f'git commit -m "{commit_message}"', date_str=commit_date_str)
        run_git_command("git push") # Assuming you want to push after each commit

        print(f"--- Finished Day {day} ---\n")

    print("\n--- All days processed successfully! ---")

# --- Run the automation ---
if __name__ == "__main__":
    # Ensure git repo is initialized
    if not os.path.exists(".git"):
        print("Git repository not found. Initializing...")
        run_git_command("git init")
        run_git_command("git branch -M main")
        print("Please configure your git remote origin (e.g., git remote add origin <repo_url>)")
        print("Then run this script again.")
        exit(0)

    # IMPORTANT: Before running this script for the first time,
    # make sure you have:
    # 1. `Obscura_Building_LLM_from_Scratch_Main.ipynb` in the same directory.
    # 2. `commit_msg.txt` populated as provided previously (with markers).
    # 3. Your GitHub remote configured: `git remote add origin <your_repo_url>`
    # 4. An initial commit (e.g., empty README or .gitignore) if the branch is empty on GitHub.

    # Example usage: process from Day 11 to Day 15 for Chapter 4
    automate_git_push(start_day=11, end_day=15) # Changed start_day to 11
