[
  {
    "day": 1,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n"
  },
  {
    "day": 2,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n"
  },
  {
    "day": 3,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n"
  },
  {
    "day": 4,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n"
  },
  {
    "day": 5,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n### Implementing Embeddings:\n\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\n"
  },
  {
    "day": 6,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n### Implementing Embeddings:\n\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\n\n## Chapter 3: Implementing Attention Mechanism\n\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\n\n### Core Self-Attention Logic:\n\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\n"
  },
  {
    "day": 7,
    "content": "# Obscura: Building an LLM from Scratch\n\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \"Attention Is All You Need\" paper.\n\n## Chapter 1: Initial Setup and Dependencies\n\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\n\n### Dependencies Installed:\n\n* `uv`: A fast Python package installer and resolver.\n\n## Chapter 2: Working with Text Data\n\nThis chapter begins the process of preparing text data for model training.\n\n### Initial Text Processing:\n\n* **Data Loading:** Loaded the raw text content from the \"Attention Is All You Need\" paper.\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\n\n### Vocabulary Creation and Custom Tokenizers:\n\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\n\n### Data Loading with Tiktoken and PyTorch Datasets:\n\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\n\n### Implementing Embeddings:\n\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\n\n## Chapter 3: Implementing Attention Mechanism\n\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\n\n### Core Self-Attention Logic:\n\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\\sqrt{d_k}$) to prevent vanishing gradients.\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n"
  },
  {
    "day": 8,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\\sqrt{d_k}$) to prevent vanishing gradients.\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\\n### Regularization with Dropout:\\n\\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n"
  },
  {
    "day": 9,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\\sqrt{d_k}$) to prevent vanishing gradients.\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\n### Regularization with Dropout:\\n\\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n\n### Stacking Multiple Single-Head Attention Layers (Multi-Head Attention Wrapper):\n\n* **`MultiHeadAttentionWrapper` Class:** Implemented a wrapper class that allows for stacking multiple independent `CausalAttention` heads.\\n* **Concatenation of Heads:** Showcased how the output context vectors from each individual attention head are concatenated along the last dimension to form a richer, combined representation.\\n* **Conceptual Multi-Head:** This serves as an initial step towards understanding multi-head attention, where different heads can learn to focus on different aspects of the input sequence.\\n"
  },
  {
    "day": 10,
    "content": "# Obscura: Building an LLM from Scratch\\n\\nThis repository documents the journey of building a Large Language Model (LLM) from scratch, inspired by the concepts presented in the \\\"Attention Is All You Need\\\" paper.\\n\\n## Chapter 1: Initial Setup and Dependencies\\n\\nThis initial commit focuses on setting up the development environment and installing necessary dependencies.\\n\\n### Dependencies Installed:\\n\\n* `uv`: A fast Python package installer and resolver.\\n\\n## Chapter 2: Working with Text Data\\n\\nThis chapter begins the process of preparing text data for model training.\\n\\n### Initial Text Processing:\\n\\n* **Data Loading:** Loaded the raw text content from the \\\"Attention Is All You Need\\\" paper.\\n* **File Storage:** Saved the raw text content to `sample_data.txt` for local access.\\n* **Basic Tokenization Exploration:** Explored initial methods for splitting text into tokens using regular expressions, handling spaces, commas, and periods.\\n* **Token Cleaning:** Refined the tokenization process to strip leading/trailing whitespace from tokens and filter out any empty strings resulting from the split.\\n* **Expanded Punctuation:** Extended the set of punctuation characters considered during splitting to include more common symbols.\\n* **Application to Raw Text:** Applied the developed tokenization logic to the full `sample_data.txt`.\\n\\n### Vocabulary Creation and Custom Tokenizers:\\n\\n* **Asterisk Inclusion:** Modified the regular expression for tokenization to correctly handle asterisk characters.\\n* **Vocabulary Generation:** Created a unique vocabulary of all tokens found in the preprocessed text and determined its size.\\n* **`SimpleTokenizerV1`:** Implemented a basic tokenizer class that maps strings to integers (`str_to_int`) and vice-versa (`int_to_str`) based on the generated vocabulary. This tokenizer can encode text into numerical IDs and decode IDs back into text.\\n* **Special Tokens:** Introduced special tokens `<|endoftext|>` (to denote the end of a text segment or document) and `<|unk|>` (for unknown tokens not present in the vocabulary).\\n* **`SimpleTokenizerV2`:** Developed an improved tokenizer that handles out-of-vocabulary words by mapping them to the `<|unk|>` token, preventing errors during encoding.\\n* **Text Concatenation:** Demonstrated how to concatenate multiple text snippets using the `<|endoftext|>` token, a common practice in LLM training for handling multiple documents or conversational turns.\\n\\n### Data Loading with Tiktoken and PyTorch Datasets:\\n\\n* **Tiktoken Integration:** Switched to `tiktoken`, a more efficient and robust tokenizer used by OpenAI's GPT models, for production-ready tokenization.\\n* **`GPTDatasetV1` Class:** Implemented a PyTorch `Dataset` class (`GPTDatasetV1`) to prepare the tokenized text data. This dataset uses a sliding window approach to create input sequences (context) and their corresponding target sequences (the next token to predict).\\n* **Context-Target Pairs:** Demonstrated how the `GPTDatasetV1` chunks the raw text into overlapping sequences, where each input sequence predicts the next token.\\n* **`create_dataloader_v1` Function:** Created a utility function to easily initialize a `DataLoader` from the `GPTDatasetV1`, allowing for batching, shuffling, and multi-process data loading.\\n* **Batch Verification:** Verified the structure and content of the input and target tensors generated by the `DataLoader`.\\n\\n### Implementing Embeddings:\\n\\n* **Token Embeddings:** Implemented a `torch.nn.Embedding` layer to convert discrete token IDs into continuous vector representations (embeddings). This layer learns a unique vector for each token in the vocabulary.\\n* **Positional Embeddings:** Implemented a separate `torch.nn.Embedding` layer for positional encodings. These embeddings provide information about the absolute or relative position of each token within a sequence, which is crucial for Transformer models that lack inherent sequential processing.\\n* **Embedding Combination (Conceptual):** Prepared the components that will later be combined (token embeddings + positional embeddings) to form the final input representation for the Transformer.\\n\n## Chapter 3: Implementing Attention Mechanism\\n\\nThis chapter begins the core implementation of the self-attention mechanism, a fundamental component of the Transformer architecture.\\n\\n### Core Self-Attention Logic:\\n\\n* **Dot Product Attention (Conceptual):** Explored the fundamental concept of attention by manually calculating attention scores using dot products between query and key vectors.\\n* **Softmax Application:** Demonstrated how the softmax function is applied to attention scores to obtain attention weights, ensuring they sum to 1 and represent probabilities. A naive softmax implementation was also compared with `torch.softmax`.\\n* **Context Vector Calculation:** Illustrated how context vectors are derived by taking a weighted sum of value vectors, where the weights are the attention weights.\\n* **Matrix Multiplication for Efficiency:** Showcased how these manual calculations can be efficiently performed using matrix multiplication (`@`) for all tokens simultaneously, leading to a much faster computation.\\n\n### Self-Attention with Trainable Weights:\\n\\n* **Trainable Weight Matrices:** Introduced the concept of trainable weight matrices ($W_Q$, $W_K$, $W_V$) to project input embeddings into query, key, and value vectors.\\n* **Query, Key, Value Projections:** Demonstrated how input embeddings are transformed into query, key, and value vectors through linear transformations.\\n* **Scaled Dot-Product Attention:** Implemented the scaled dot-product attention mechanism, including the scaling factor ($\\sqrt{d_k}$) to prevent vanishing gradients.\\n* **`SelfAttention_v1` Class:** Created a PyTorch `nn.Module` for a basic `SelfAttention_v1` layer using `nn.Parameter` for the weight matrices.\\n* **`SelfAttention_v2` Class (using `nn.Linear`):** Improved the `SelfAttention` implementation by using `torch.nn.Linear` layers, which automatically handle weights and biases.\\n* **Causal Masking:** Introduced the concept of causal (or look-ahead) masking, essential for generative models. This ensures that a token can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\\n* **Masking Implementation:** Demonstrated how to create a lower triangular mask (`torch.tril`) and apply it to attention scores using `masked_fill_` to set future attention scores to negative infinity before softmax.\\n\n### Regularization with Dropout:\\n\\n* **Dropout Mechanism:** Explored the `torch.nn.Dropout` layer, a common regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero at each update during training.\\n* **`CausalAttention` Class:** Implemented a comprehensive `CausalAttention` class that integrates all the previously discussed components: QKV linear projections, scaled dot-product attention, causal masking, and dropout. This class represents a single head of causal self-attention.\\n* **Batch Processing:** Demonstrated the `CausalAttention` class's ability to process batched inputs, a crucial aspect for efficient deep learning training.\\n\n### Multi-Head Attention:\\n\\n* **`MultiHeadAttentionWrapper` (Conceptual):** An initial approach to multi-head attention by simply stacking multiple independent `CausalAttention` heads and concatenating their outputs. This illustrates the idea of multiple \"perspectives.\"\n* **Efficient `MultiHeadAttention` Class:** Implemented the more standard and efficient `MultiHeadAttention` class. This class projects the input into a single large Q, K, and V matrix, then implicitly splits these into multiple heads using `view` and `transpose` operations.\n* **Combined Output Projection:** Included an `out_proj` linear layer to combine the concatenated outputs from all attention heads back into the desired output dimension.\\n* **Demonstration:** Verified the functionality of the `MultiHeadAttention` class with sample batch data, confirming the correct output shape and demonstrating the parallel computation across heads.\n"
  }
]